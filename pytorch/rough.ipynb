{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6cb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95e9cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'runs/rough/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3977920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "IMG_WIDTH=64\n",
    "IMG_HEIGHT=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "457c454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_folder):\n",
    "   \n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "   \n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            d = cv2.imread(image_path)\n",
    "            image= cv2.cvtColor( d, cv2.COLOR_BGR2GRAY)\n",
    "            image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "            image=np.array(image)\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    return img_data_array, class_name\n",
    "# extract the image array and class name\n",
    "img_data, class_name = create_dataset('archive/seg_train/seg_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcbeea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgdataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_samples = x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d20649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14034, 4096]) torch.Size([14034])\n"
     ]
    }
   ],
   "source": [
    "img_data = np.asarray(img_data,dtype=np.float32)\n",
    "img_data = torch.FloatTensor(img_data)\n",
    "img_data = img_data.reshape(-1,IMG_WIDTH*IMG_HEIGHT)\n",
    "target_dict={k: v for v, k in enumerate(np.unique(class_name))}\n",
    "target_val=  [target_dict[class_name[i]] for i in range(len(class_name))]\n",
    "target_val = np.array(target_val,dtype=np.longlong)\n",
    "target_val = torch.LongTensor(target_val)\n",
    "print(img_data.shape, target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38d33eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = img_data.shape[1]\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "n_classes = 6\n",
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55ce2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imgdataset(img_data,target_val)\n",
    "X_dataloader = DataLoader(dataset=data,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "582eb0ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096]) torch.Size([64])\n",
      "tensor([0.9961, 0.9961, 0.9961,  ..., 0.1686, 0.2039, 0.2196])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(X_dataloader)\n",
    "p,q = examples.next()\n",
    "print(p.shape,q.shape)\n",
    "print(p[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d86dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnmodel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(nnmodel, self).__init__()\n",
    "        self.r = nn.ReLU()\n",
    "        self.l1 = nn.Linear(input_dim,128)\n",
    "        self.l2 = nn.Linear(128,64)\n",
    "        self.l3 = nn.Linear(64,32)\n",
    "        self.l4 = nn.Linear(32,16)\n",
    "        self.l5 = nn.Linear(16,8)\n",
    "        self.pred = nn.Linear(8,output_dim)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.r(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.r(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.r(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.r(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.r(out)\n",
    "        out = self.r(self.pred(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2eb30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnmodel1(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(nnmodel1, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim,100)\n",
    "        self.t1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(100,100)\n",
    "        self.t2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(100,100)\n",
    "        self.t3 = nn.ReLU()\n",
    "        self.l4 = nn.Linear(100,100)\n",
    "        self.t4 = nn.ReLU()\n",
    "        self.l5 = nn.Linear(100,100)\n",
    "        self.t5 = nn.ReLU()\n",
    "        self.pred = nn.Linear(100,output_dim)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.t1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.t2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.t3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.t4(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.t5(out)\n",
    "        out = self.pred(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3cc864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnmodel2(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(nnmodel2, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim,64*64)\n",
    "        #self.l1.weight = torch.nn.init.xavier_normal_(self.l1.weight,1.0)\n",
    "        #print(self.l1.weight)\n",
    "        self.t1 = nn.LeakyReLU()\n",
    "        self.l5 = nn.Linear(64*64,32*32)\n",
    "        #self.l5.weight = torch.nn.init.kaiming_normal_(self.l5.weight)\n",
    "        #print(self.l5.weight)\n",
    "        self.t5 = nn.LeakyReLU()\n",
    "        self.l2 = nn.Linear(32*32,16*16)\n",
    "        #self.l2.weight = torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        #print(self.l2.weight)\n",
    "        self.t2 = nn.LeakyReLU()\n",
    "        self.l3 = nn.Linear(16*16,8*8)\n",
    "        #self.l3.weight = torch.nn.init.kaiming_uniform_(self.l3.weight)\n",
    "        self.t3 = nn.LeakyReLU()\n",
    "        self.l4 = nn.Linear(8*8,4*4)\n",
    "        #self.l4.weight = torch.nn.init.xavier_normal_(self.l4.weight)\n",
    "        self.t4 = nn.LeakyReLU()\n",
    "        self.pred = nn.Linear(4*4,output_dim)\n",
    "        #self.pred.weight = torch.nn.init.kaiming_normal_(self.pred.weight)\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.t1(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.t5(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.t2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.t3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.t4(out)\n",
    "        out = self.pred(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "494ec5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0055, -0.0072, -0.0047,  ...,  0.0072,  0.0100,  0.0126],\n",
      "        [ 0.0151,  0.0015,  0.0099,  ..., -0.0047, -0.0049, -0.0137],\n",
      "        [-0.0101, -0.0101, -0.0152,  ...,  0.0004, -0.0021, -0.0114],\n",
      "        ...,\n",
      "        [ 0.0129,  0.0092, -0.0026,  ..., -0.0106, -0.0093, -0.0137],\n",
      "        [-0.0025, -0.0121,  0.0113,  ...,  0.0025,  0.0034,  0.0006],\n",
      "        [ 0.0126, -0.0130, -0.0065,  ...,  0.0080,  0.0057, -0.0022]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0135, -0.0140, -0.0009,  ...,  0.0107,  0.0152,  0.0015],\n",
      "        [ 0.0056,  0.0083,  0.0144,  ...,  0.0113,  0.0142,  0.0084],\n",
      "        [-0.0113, -0.0021, -0.0132,  ...,  0.0043,  0.0151, -0.0115],\n",
      "        ...,\n",
      "        [-0.0056, -0.0134, -0.0057,  ...,  0.0111,  0.0018, -0.0089],\n",
      "        [-0.0076, -0.0102,  0.0088,  ..., -0.0155, -0.0074, -0.0019],\n",
      "        [-0.0029, -0.0040,  0.0097,  ..., -0.0011, -0.0012, -0.0087]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0081,  0.0136, -0.0168,  ...,  0.0221,  0.0150, -0.0019],\n",
      "        [-0.0202, -0.0020, -0.0253,  ...,  0.0141, -0.0282,  0.0193],\n",
      "        [ 0.0230,  0.0277,  0.0096,  ..., -0.0186, -0.0217, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0106,  0.0016, -0.0260,  ...,  0.0012, -0.0211, -0.0303],\n",
      "        [ 0.0134, -0.0282,  0.0305,  ..., -0.0103,  0.0113, -0.0130],\n",
      "        [ 0.0239, -0.0140,  0.0298,  ..., -0.0153, -0.0261,  0.0085]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nnmodel2(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e59e611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, p[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25b41be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dbb2b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs :1/5, step: 50/220, loss : 1.824745774269104\n",
      "epochs :1/5, step: 100/220, loss : 1.735074520111084\n",
      "epochs :1/5, step: 150/220, loss : 1.6869785785675049\n",
      "epochs :1/5, step: 200/220, loss : 1.52239191532135\n",
      "epochs :2/5, step: 50/220, loss : 1.7479628324508667\n",
      "epochs :2/5, step: 100/220, loss : 1.608888864517212\n",
      "epochs :2/5, step: 150/220, loss : 1.658441185951233\n",
      "epochs :2/5, step: 200/220, loss : 1.6477131843566895\n",
      "epochs :3/5, step: 50/220, loss : 1.6866381168365479\n",
      "epochs :3/5, step: 100/220, loss : 1.4931752681732178\n",
      "epochs :3/5, step: 150/220, loss : 1.6046814918518066\n",
      "epochs :3/5, step: 200/220, loss : 1.5873149633407593\n",
      "epochs :4/5, step: 50/220, loss : 1204.036376953125\n",
      "epochs :4/5, step: 100/220, loss : 16264.9130859375\n",
      "epochs :4/5, step: 150/220, loss : 15624.27734375\n",
      "epochs :4/5, step: 200/220, loss : 3326.083251953125\n",
      "epochs :5/5, step: 50/220, loss : 195.35067749023438\n",
      "epochs :5/5, step: 100/220, loss : 134.62899780273438\n",
      "epochs :5/5, step: 150/220, loss : 34.630348205566406\n",
      "epochs :5/5, step: 200/220, loss : 3.8163676261901855\n"
     ]
    }
   ],
   "source": [
    "n_total_epochs = len(X_dataloader)\n",
    "for epoch in range(n_epoch):\n",
    "    for i, (images,labels) in enumerate(X_dataloader):\n",
    "        outputs = model(images)\n",
    "        loss = loss_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i+1) % 50 ==0 :\n",
    "            print(f'epochs :{epoch+1}/{n_epoch}, step: {i+1}/{n_total_epochs}, loss : {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "376c5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_classes = create_dataset('archive/seg_test/seg_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4106ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 4096])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_img))\n",
    "test_img = np.asarray(test_img)\n",
    "test_img = torch.FloatTensor(test_img)\n",
    "test_img = test_img.reshape(-1,IMG_WIDTH*IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c28c2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcec40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_dict = {k: v for v,k in enumerate(np.unique(test_classes))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8133bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [test_labels_dict[class_name[i]] for i in range(len(test_classes))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b1720f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.asarray(test_labels,dtype=np.longlong)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66abe705",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = imgdataset(test_img,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e379716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_loader,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f866d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_epochs_test = len(test_loader)\n",
    "for epoch in range(n_epoch):\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        outputs = model(images)\n",
    "        loss1 = loss_criterion(outputs, labels)\n",
    "        if (i+1) % 1000 ==0 :\n",
    "            print(f'epochs :{epoch+1}/{n_epoch}, step: {i+1}/{n_total_epochs_test}, loss : {loss1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1021f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "with torch.no_grad():\n",
    "    n_correct=0\n",
    "    n_samples=0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        outputs = m(outputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples+=labels.shape[0]\n",
    "        n_correct = (predictions == labels).sum().item()\n",
    "    acc = 100.0 * n_correct/n_samples\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2e1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
